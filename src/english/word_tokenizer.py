# Copyright (c) 2025 Ming Yu (yuming@oppo.com), Liangliang Han (hanliangliang@oppo.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±æ–‡è¯çº§åˆ†è¯å™¨å’ŒSymbolTableç®¡ç†

å°†è‹±æ–‡æ–‡æœ¬ä»å­—ç¬¦çº§è½¬æ¢ä¸ºè¯çº§å¤„ç†ï¼Œå¤§å¹…å‡å°‘FST tokenæ•°é‡ã€‚
"""

import pynini
import re
import json
import os
from typing import List, Optional
from .global_symbol_table import get_symbol_table


class EnglishWordTokenizer:
    """
    è‹±æ–‡è¯çº§åˆ†è¯å™¨

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç»´æŠ¤è¯æ±‡è¡¨ï¼ˆSymbolTableï¼‰
    2. å°†æ–‡æœ¬åˆ†è¯ä¸ºè¯åˆ—è¡¨
    3. æ„å»ºè¯çº§è¾“å…¥FST
    """

    def __init__(self, vocab_file: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨

        Args:
            vocab_file: è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨vocabulary_complete.txtï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨å…¨å±€SymbolTableï¼‰
        """
        # ä½¿ç”¨å…¨å±€SymbolTableï¼ˆå•ä¾‹ï¼‰
        self.sym = get_symbol_table()

        # ä»SymbolTableæ„å»ºè¯æ±‡è¡¨
        self.vocab = set()
        for i in range(self.sym.num_symbols()):
            symbol = self.sym.find(i)
            if symbol and symbol.isalpha():
                self.vocab.add(symbol)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_words": 0,
            "unknown_words": 0,
            "char_level_fallback": 0,
            "long_number_tokens": 0,
        }
        self.long_number_threshold = 6  # >=6 ä½çš„æ•°å­—è§†ä¸ºé•¿æ•°å­—ï¼Œé¿å…è†¨èƒ€
        self.long_number_token = "__long_number__"
        if self.sym.find(self.long_number_token) == -1:
            self.sym.add_symbol(self.long_number_token)

    def tokenize(self, text: str) -> List[str]:  # noqa: C901
        """
        å°†æ–‡æœ¬åˆ†è¯ä¸ºè¯åˆ—è¡¨

        ç­–ç•¥ï¼š
        - å¸¦è¿å­—ç¬¦/ç‚¹çš„è¯ï¼šå°è¯•ä½œä¸ºæ•´ä½“
        - å­—æ¯åºåˆ—ï¼šä½œä¸ºæ•´è¯ï¼Œå¦‚æœä¸åœ¨è¯æ±‡è¡¨åˆ™æ‹†æˆå­—ç¬¦
        - è¿ç»­æ•°å­—ï¼šä½œä¸ºæ•´ä½“
        - æ ‡ç‚¹ï¼šä¿æŒå•å­—ç¬¦
        - ç©ºæ ¼ï¼šä¿ç•™ï¼ˆç”¨äºè¯é—´åˆ†éš”ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            è¯tokenåˆ—è¡¨
        """
        text = text.lower().strip()
        tokens = []

        # æ”¹è¿›çš„æ­£åˆ™ï¼šå…ˆåŒ¹é…æ‰€æœ‰æ ¼å½¢å¼ï¼ˆword'sï¼‰ï¼Œå†åŒ¹é…æ™®é€šå•è¯
        # åŒ¹é…é¡ºåºï¼š1) æ‰€æœ‰æ ¼å½¢å¼ï¼ˆword'sï¼‰ 2) æ™®é€šå•è¯ 3) æ•°å­— 4) æ ‡ç‚¹ 5) ç©ºæ ¼
        # æ³¨æ„ï¼šæ­£åˆ™è¡¨è¾¾å¼éœ€è¦èƒ½åŒ¹é…åŒ…å«å•å¼•å·çš„å•è¯ï¼Œç„¶ååå¤„ç†æ‹†åˆ†
        pattern = r"[a-z]+(?:'[a-z]+)?(?:[-.][a-z]+(?:'[a-z]+)?)*|[0-9]+|[^\w\s]|\s+"
        matches = re.findall(pattern, text)

        # åå¤„ç†ï¼šæ£€æŸ¥åŒ¹é…ç»“æœï¼Œå¦‚æœåŒ…å«æ‰€æœ‰æ ¼ï¼Œéœ€è¦æ‹†åˆ†ä¸ºbase_wordå’Œ"'s"
        processed_matches = []
        for match in matches:
            if match and not match.isspace() and match[0].isalpha() and "'" in match:
                # åŒ…å«æ‰€æœ‰æ ¼çš„å•è¯ï¼Œæ‹†åˆ†ä¸ºbase_wordå’Œpossessive
                # ä¾‹å¦‚ï¼š"year's" -> "year" + "'s"
                parts = match.rsplit("'", 1)
                base_word = parts[0]
                possessive_suffix = parts[1] if len(parts) > 1 else ""

                if base_word:
                    processed_matches.append(base_word)
                if possessive_suffix:
                    # å°†"'s"ä½œä¸ºä¸€ä¸ªæ•´ä½“token
                    possessive = "'" + possessive_suffix
                    processed_matches.append(possessive)
            else:
                processed_matches.append(match)
        matches = processed_matches

        for match in matches:
            if match.isspace():
                # ä¿ç•™ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦
                tokens.append(" ")
            elif match[0].isalpha():
                # å­—æ¯åºåˆ—ï¼ˆå¯èƒ½å¸¦æ‰€æœ‰æ ¼ã€è¿å­—ç¬¦ç­‰ï¼‰
                # å…ˆå°è¯•ä½œä¸ºæ•´ä½“æŸ¥æ‰¾ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ ¼å½¢å¼ï¼‰
                if self.sym.find(match) != -1:
                    tokens.append(match)
                    self.stats["total_words"] += 1
                elif match in self.vocab:
                    tokens.append(match)
                    self.stats["total_words"] += 1
                else:
                    # ä¸åœ¨è¯æ±‡è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æ ¼
                    if "'" in match:
                        # å¤„ç†æ‰€æœ‰æ ¼ï¼šæ€»æ˜¯æ‹†åˆ†ä¸ºbase_word + possessive
                        # ä¾‹å¦‚ï¼š"tomorrow's" -> "tomorrow" + "'s"
                        parts = match.rsplit("'", 1)
                        base_word = parts[0]  # "tomorrow's" -> "tomorrow"
                        possessive_suffix = parts[1] if len(parts) > 1 else ""
                        possessive = "'" + possessive_suffix if possessive_suffix else "'s"

                        # å¤„ç†base_wordï¼šä¼˜å…ˆä½œä¸ºæ•´ä½“ï¼Œé¿å…å­—ç¬¦çº§æ‹†åˆ†
                        if base_word:
                            if self.sym.find(base_word) != -1:
                                tokens.append(base_word)
                                self.stats["total_words"] += 1
                            elif base_word in self.vocab:
                                tokens.append(base_word)
                                self.stats["total_words"] += 1
                            else:
                                # base_wordä¸åœ¨ç¬¦å·è¡¨ï¼Œä½†ä¸è¦ç«‹å³æ‹†åˆ†
                                # å…ˆå°è¯•æ·»åŠ åˆ°vocabï¼ˆå¦‚æœçœ‹èµ·æ¥åƒå•è¯ï¼‰
                                if base_word.isalpha() and len(base_word) > 1:
                                    # çœ‹èµ·æ¥åƒå•è¯ï¼Œå°è¯•ç›´æ¥æ·»åŠ ï¼ˆå¯èƒ½SymbolTableä¼šåŠ¨æ€æ·»åŠ ï¼‰
                                    tokens.append(base_word)
                                    self.stats["total_words"] += 1
                                else:
                                    # æœ€ç»ˆå›é€€ï¼šå­—ç¬¦çº§
                                    tokens.extend(list(base_word))
                                    self.stats["char_level_fallback"] += len(base_word)

                        # æ·»åŠ æ‰€æœ‰æ ¼åç¼€ï¼ˆä¼˜å…ˆä½¿ç”¨æ ‡å‡†å½¢å¼"'s"ï¼‰
                        if possessive:
                            # å°è¯•æ ‡å‡†å½¢å¼"'s"
                            standard_possessive = "'s"
                            if self.sym.find(standard_possessive) != -1:
                                tokens.append(standard_possessive)
                            elif self.sym.find(possessive) != -1:
                                tokens.append(possessive)
                            else:
                                # å¦‚æœéƒ½ä¸åœ¨ï¼Œå°è¯•æ·»åŠ æ ‡å‡†å½¢å¼ï¼ˆå¯èƒ½SymbolTableä¼šå¤„ç†ï¼‰
                                tokens.append(standard_possessive)
                                # è®°å½•ä½†ä¸å›é€€åˆ°å­—ç¬¦çº§ï¼ˆæ‰€æœ‰æ ¼åç¼€åº”è¯¥æ˜¯æ ‡å‡†å½¢å¼ï¼‰
                    elif "-" in match or "." in match:
                        # æ‹†åˆ†å¤åˆè¯
                        parts = re.split(r"([-'.])", match)
                        for part in parts:
                            if part and self.sym.find(part) != -1:
                                tokens.append(part)
                            elif part and part in self.vocab:
                                tokens.append(part)
                            elif part:
                                # æœ€ç»ˆå›é€€ï¼šå­—ç¬¦çº§
                                tokens.extend(list(part))
                                self.stats["char_level_fallback"] += len(part)
                    else:
                        # å•çº¯çš„æœªçŸ¥è¯
                        # å¦‚æœçœ‹èµ·æ¥åƒå•è¯ï¼ˆå…¨å­—æ¯ä¸”é•¿åº¦>1ï¼‰ï¼Œä¸”ä¸åœ¨SymbolTableä¸­
                        # éœ€è¦æ‹†åˆ†ä¸ºå­—ç¬¦ï¼Œå› ä¸ºè¯çº§FSTéœ€è¦æ‰€æœ‰tokenéƒ½åœ¨SymbolTableä¸­
                        if match.isalpha() and len(match) > 1:
                            # æ£€æŸ¥æ˜¯å¦åœ¨SymbolTableä¸­
                            if self.sym.find(match) != -1:
                                # åœ¨SymbolTableä¸­ï¼Œä½œä¸ºæ•´ä½“ä¿ç•™
                                tokens.append(match)
                                self.stats["total_words"] += 1
                            else:
                                # ä¸åœ¨SymbolTableä¸­ï¼Œæ‹†åˆ†ä¸ºå­—ç¬¦ï¼ˆè¯çº§FSTè¦æ±‚æ‰€æœ‰tokenéƒ½åœ¨SymbolTableä¸­ï¼‰
                                tokens.extend(list(match))
                                self.stats["unknown_words"] += 1
                                self.stats["char_level_fallback"] += len(match)
                        else:
                            # æœ€ç»ˆå›é€€ï¼šå­—ç¬¦çº§
                            tokens.extend(list(match))
                            self.stats["unknown_words"] += 1
                            self.stats["char_level_fallback"] += len(match)
            elif match[0].isdigit():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åºæ•°åç¼€ï¼ˆst, nd, rd, thï¼‰
                # ä¾‹å¦‚ï¼š"20th" -> ["20", "th"], "2nd" -> ["2", "nd"]
                ordinal_match = re.match(r"^(\d+)(st|nd|rd|th)$", match, re.IGNORECASE)
                if ordinal_match:
                    number = ordinal_match.group(1)
                    suffix = ordinal_match.group(2).lower()  # ç»Ÿä¸€å°å†™

                    # å¤„ç†æ•°å­—éƒ¨åˆ†
                    if len(number) >= self.long_number_threshold or self.sym.find(number) == -1:
                        tokens.append(self.long_number_token)
                        self.stats["long_number_tokens"] += 1
                    else:
                        tokens.append(number)

                    # å¤„ç†åºæ•°åç¼€ï¼ˆåº”è¯¥å·²åœ¨symbol tableä¸­ï¼‰
                    if self.sym.find(suffix) != -1:
                        tokens.append(suffix)
                    else:
                        # å¦‚æœåç¼€ä¸åœ¨symbol tableï¼Œå°è¯•æ·»åŠ ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                        tokens.append(suffix)
                        self.stats["char_level_fallback"] += len(suffix)
                else:
                    # æ™®é€šæ•°å­—ï¼Œä¸åŒ…å«åºæ•°åç¼€
                    # è¿ç»­æ•°å­—ä½œä¸ºæ•´ä½“
                    if len(match) >= self.long_number_threshold or self.sym.find(match) == -1:
                        tokens.append(self.long_number_token)
                        self.stats["long_number_tokens"] += 1
                    else:
                        tokens.append(match)
            else:
                # æ ‡ç‚¹ï¼ˆåŒ…æ‹¬å†’å·ç­‰ï¼‰
                # ç‰¹æ®Šå¤„ç†ï¼šæ—¶é—´æ ¼å¼ä¸­çš„å†’å·ï¼ˆå¦‚"8:30"ï¼‰åº”è¯¥åœ¨åˆ†è¯æ—¶ä¿æŒ
                # ä½†è¿™é‡Œmatchå·²ç»æ˜¯å•ä¸ªå­—ç¬¦ï¼ˆå†’å·ï¼‰ï¼Œæ‰€ä»¥ç›´æ¥å¤„ç†
                if self.sym.find(match) != -1:
                    tokens.append(match)
                else:
                    # ğŸ”§ å¯¹é½ä¸­æ–‡tokenizerï¼šä¸åœ¨ç¬¦å·è¡¨ä¸­çš„å­—ç¬¦ï¼Œå°è¯•æ‹†åˆ†ä¸ºå­—ç¬¦
                    # å¦‚æœæ‹†åˆ†åä»ç„¶æ²¡æœ‰åœ¨ç¬¦å·è¡¨ä¸­çš„å­—ç¬¦ï¼Œåˆ™è¿‡æ»¤æ‰ï¼ˆä¸æ·»åŠ ï¼‰
                    fallback_chars = [ch for ch in match if self.sym.find(ch) != -1]
                    if fallback_chars:
                        tokens.extend(fallback_chars)
                        self.stats["char_level_fallback"] += len(fallback_chars)
                    else:
                        # å®Œå…¨æœªçŸ¥çš„å­—ç¬¦ï¼Œè¿‡æ»¤æ‰ï¼ˆå‚è€ƒä¸­æ–‡tokenizerï¼‰
                        self.stats["unknown_tokens"] = self.stats.get("unknown_tokens", 0) + 1
                    # ä¸å†æ— æ¡ä»¶æ·»åŠ unknown token

        return tokens

    def build_input_fst(self, tokens: List[str]) -> pynini.Fst:
        """
        å°†è¯tokenåˆ—è¡¨è½¬æ¢ä¸ºè¯çº§FST

        Args:
            tokens: è¯tokenåˆ—è¡¨

        Returns:
            è¯çº§FST
        """
        if not tokens:
            # ç©ºè¾“å…¥ï¼Œè¿”å›epsilon
            fst = pynini.Fst()
            s = fst.add_state()
            fst.set_start(s)
            fst.set_final(s)
            fst.set_input_symbols(self.sym)
            fst.set_output_symbols(self.sym)
            return fst

        # æ‰‹åŠ¨æ„å»ºFST
        fst = pynini.Fst()
        fst.set_input_symbols(self.sym)
        fst.set_output_symbols(self.sym)

        # åˆ›å»ºçŠ¶æ€é“¾
        states = [fst.add_state() for _ in range(len(tokens) + 1)]
        fst.set_start(states[0])
        fst.set_final(states[-1])

        # ä¸ºæ¯ä¸ªtokenæ·»åŠ arc
        # æ³¨æ„ï¼šå¦‚æœtokenä¸åœ¨SymbolTableä¸­ï¼Œåº”è¯¥åœ¨tokenize()é˜¶æ®µå°±æ‹†åˆ†ä¸ºå­—ç¬¦
        # è¿™é‡Œåªæ˜¯æ„å»ºFSTï¼Œä¸è¿›è¡Œæ‹†åˆ†
        for i, token in enumerate(tokens):
            token_id = self.sym.find(token)
            if token_id == -1:
                # æœªåœ¨SymbolTableä¸­ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿï¼ˆåº”è¯¥åœ¨tokenize()é˜¶æ®µå¤„ç†ï¼‰
                # ä½†ä¸ºäº†å¥å£®æ€§ï¼Œæˆ‘ä»¬è®°å½•è­¦å‘Šå¹¶è·³è¿‡è¿™ä¸ªtoken
                import logging

                logger = logging.getLogger("fst_time")
                logger.warning(
                    f"Token '{token}' not in SymbolTable, skipping. Should be split into characters in tokenize()"
                )
                continue

            # æ·»åŠ arc: state[i] -> state[i+1], label=token
            arc = pynini.Arc(
                token_id, token_id, pynini.Weight.one(fst.weight_type()), states[i + 1]
            )
            fst.add_arc(states[i], arc)

        return fst

    def process_text(self, text: str) -> pynini.Fst:
        """
        å¤„ç†æ–‡æœ¬ï¼šåˆ†è¯ + æ„å»ºFST

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            è¯çº§FST
        """
        tokens = self.tokenize(text)
        return self.build_input_fst(tokens)

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {"total_words": 0, "unknown_words": 0, "char_level_fallback": 0}


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åˆ†è¯å™¨
    print("=" * 80)
    print("æµ‹è¯•EnglishWordTokenizer")
    print("=" * 80)
    print()

    try:
        tokenizer = EnglishWordTokenizer()

        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            "tomorrow",
            "next monday",
            "remind me at 3:30",
            "what is the time",
            "schedule meeting for friday",
        ]

        print("æµ‹è¯•åˆ†è¯:")
        print("-" * 80)
        for text in test_cases:
            tokens = tokenizer.tokenize(text)
            char_count = len(text)
            token_count = len(tokens)
            reduction = (char_count - token_count) / char_count * 100

            print(f'æ–‡æœ¬: "{text}"')
            print(f"  å­—ç¬¦æ•°: {char_count}")
            print(f"  Tokenæ•°: {token_count}")
            print(f"  å‡å°‘: {reduction:.1f}%")
            print(f"  Tokens: {tokens}")
            print()

        # æµ‹è¯•FSTæ„å»º
        print("æµ‹è¯•FSTæ„å»º:")
        print("-" * 80)
        text = "next monday"
        tokens = tokenizer.tokenize(text)
        fst = tokenizer.build_input_fst(tokens)

        print(f'æ–‡æœ¬: "{text}"')
        print(f"Tokens: {tokens}")
        print(f"FSTçŠ¶æ€æ•°: {fst.num_states()}")
        print(f"æœŸæœ›çŠ¶æ€æ•°: {len(tokens) + 1}")
        print(f'åŒ¹é…: {"âœ“" if fst.num_states() == len(tokens) + 1 else "âœ—"}')
        print()

        # ç»Ÿè®¡ä¿¡æ¯
        print("ç»Ÿè®¡ä¿¡æ¯:")
        print("-" * 80)
        stats = tokenizer.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
