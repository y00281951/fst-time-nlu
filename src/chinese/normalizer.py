# Copyright (c) 2025 Ming Yu (yuming@oppo.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import logging
from typing import List, Dict, Any

import pynini
from pynini.lib.pynutil import add_weight
from importlib_resources import files

from ..core.processor import Processor
from ..core.logger import get_logger
from .global_symbol_table import get_symbol_table, get_input_tokens
from .word_tokenizer import ChineseWordTokenizer
from .rules import PreProcessor
from .rules.and_rule import AndRule
from .rules import (
    BetweenRule,
    DeltaRule,
    HolidayRule,
    LunarRule,
    PeriodRule,
    RelativeRule,
    UTCTimeRule,
    WeekRule,
    WhitelistRule,
    DecimalRule,
    UnitRule,
    VerbDurationRule,
    RangeRule,
    DeltaTimeAttachRule,
    RecurringRule,
)


class Normalizer(Processor):
    def __init__(
        self,
        cache_dir=None,
        overwrite_cache=False,
        remove_interjections=True,
        traditional_to_simple=True,
        remove_puncts=False,
        full_to_half=True,
        tag_oov=False,
        use_word_level: bool = True,
    ):
        super().__init__(name="zh_normalizer")
        self.logger = get_logger(__name__)
        self.remove_interjections = remove_interjections
        self.traditional_to_simple = traditional_to_simple
        self.remove_puncts = remove_puncts
        self.full_to_half = full_to_half
        self.tag_oov = tag_oov
        self.use_word_level = use_word_level
        self.word_tokenizer = None  # å…ˆè®¾ä¸ºNoneï¼Œåœ¨build_fståå†åˆå§‹åŒ–

        if cache_dir is None:
            cache_dir = str(files("src.chinese.test").joinpath("fst"))
        self.build_fst("zh_tn", cache_dir, overwrite_cache)

        # åœ¨build_fstä¹‹ååˆå§‹åŒ–word_tokenizerï¼Œç¡®ä¿ä½¿ç”¨æ‰©å±•åçš„ç¬¦å·è¡¨
        if use_word_level:
            try:
                self.word_tokenizer = ChineseWordTokenizer()
                self.logger.info(
                    f"ä¸­æ–‡è¯çº§FSTå·²å¯ç”¨ï¼ŒSymbolTableå¤§å°: {self.word_tokenizer.sym.num_symbols()}"
                )
            except Exception as e:
                self.logger.warning(f"ä¸­æ–‡è¯çº§FSTåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦çº§: {e}")
                self.word_tokenizer = None

    def tag(self, text: str) -> List[Dict[str, Any]]:
        if not text:
            return []
        result = super().tag(text)
        if result:
            return result
        # è¯çº§è§„åˆ™æœªåŒ¹é…ï¼Œå›é€€ä¸ºé€å­—ç¬¦è¾“å‡º
        fallback = []
        for ch in text:
            if ch.isspace():
                continue
            fallback.append({"type": "char", "value": ch})
        return fallback

    def _tag_single(self, text: str) -> List[Dict[str, Any]]:  # noqa: C901
        if self.word_tokenizer:
            try:
                escaped_text = self.word_tokenizer.process_text(text)

                input_sym = escaped_text.input_symbols()
                tagger_input_sym = self.tagger.input_symbols()

                if input_sym and not tagger_input_sym:
                    escaped_text = pynini.accep(text)
                elif (
                    input_sym
                    and tagger_input_sym
                    and input_sym.num_symbols() != tagger_input_sym.num_symbols()
                ):
                    escaped_text = pynini.accep(text)
            except Exception as e:
                logger = logging.getLogger(f"fst_time-{self.name}")
                logger.debug(f"è¯çº§å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦çº§: {e}")
                escaped_text = pynini.accep(text)
        else:
            escaped_text = pynini.accep(text)

        try:
            lattice = escaped_text @ self.tagger
            if lattice.num_states() == 0:
                return []

            from pynini import shortestpath

            shortest = shortestpath(lattice, nshortest=1)
            if shortest.num_states() == 0:
                return []

            if self.word_tokenizer and shortest.output_symbols():
                try:
                    sym = shortest.output_symbols()
                    tagged_text = shortest.string(token_type=sym)
                except Exception as e:
                    logger = logging.getLogger(f"fst_time-{self.name}")
                    logger.warning(f"è¯çº§FST string(token_type=sym)å¤±è´¥: {e}, æ–‡æœ¬: {text[:50]}")
                    return []
            else:
                try:
                    tagged_text = shortest.string()
                except Exception as e:
                    logger = logging.getLogger(f"fst_time-{self.name}")
                    logger.warning(f"FST string()å¤±è´¥: {e}, æ–‡æœ¬: {text[:50]}")
                    return []

            if not tagged_text:
                return []

            return self.parse_tags(tagged_text)
        except Exception as e:
            logger = logging.getLogger(f"fst_time-{self.name}")
            logger.warning(f"FSTåŒ¹é…å¤±è´¥: {e}, æ–‡æœ¬: {text[:50]}")
            return []

    def build_tagger(self):
        # ä¸´æ—¶ç¦ç”¨é¢„å¤„ç†ï¼ˆç¹ä½“è½¬ç®€ã€æ ‡ç‚¹ç­‰ï¼‰ï¼Œç›´æ¥ä½¿ç”¨è§„åˆ™ç»„åˆ
        # processor = PreProcessor(
        #     traditional_to_simple=self.traditional_to_simple).processor

        # é˜¶æ®µ1ä¼˜åŒ–ï¼šè°ƒæ•´æƒé‡å’Œé¡ºåºï¼ŒæŒ‰åŒ¹é…é¢‘ç‡æ’åº
        # æƒé‡è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼ˆpyniniçš„shortestpathé€‰æ‹©æƒé‡æœ€å°çš„è·¯å¾„ï¼‰
        utctime = add_weight(
            UTCTimeRule().tagger, 0.90
        )  # æœ€é«˜ä¼˜å…ˆçº§ï¼šå¹´æœˆæ—¥æ—¶åˆ†ç§’ï¼ˆ31.8%+30.7%+15.5%ï¼‰
        relative_day = add_weight(RelativeRule().tagger, 0.91)  # é«˜ä¼˜å…ˆçº§ï¼šæ˜å¤©ã€æ˜¨å¤©ç­‰
        period = add_weight(PeriodRule().tagger, 0.92)  # é«˜ä¼˜å…ˆçº§ï¼šä¸Šåˆã€ä¸‹åˆã€æ™šä¸Šï¼ˆ12.1%ï¼‰
        delta = add_weight(DeltaRule().tagger, 0.93)  # é«˜ä¼˜å…ˆçº§ï¼šXå¤©å‰ã€Xå°æ—¶åï¼ˆ11.0%+8.5%ï¼‰
        weekday = add_weight(WeekRule().tagger, 0.94)  # å¸¸è§ï¼šå‘¨ä¸€ã€æ˜ŸæœŸäºŒ
        between = add_weight(BetweenRule().tagger, 0.95)  # æ—¶é—´åŒºé—´
        holiday = add_weight(
            HolidayRule().tagger, 0.96
        )  # èŠ‚å‡æ—¥ï¼ˆæé«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿"å¤§å¹´ä¸‰å"ä¼˜å…ˆåŒ¹é…ä¸ºèŠ‚å‡æ—¥ï¼‰
        lunar = add_weight(LunarRule().tagger, 0.97)  # å†œå†
        recurring = add_weight(RecurringRule().tagger, 0.98)  # å‘¨æœŸè§„åˆ™
        range_rule = add_weight(
            RangeRule().tagger, 0.915
        )  # æé«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿"æœ€è¿‘çš„+X+å•ä½"ä¼˜å…ˆåŒ¹é…ä¸ºtime_rangeï¼ˆé«˜äºperiodçš„0.92ï¼‰  # èŒƒå›´è§„åˆ™
        unit = add_weight(UnitRule().tagger, 1.00)  # å•ä½ï¼ˆç¡®ä¿"æ•°å­—-æ•°å­—+å•ä½"ä¼˜å…ˆåŒ¹é…ï¼‰
        delta_time_attach = add_weight(DeltaTimeAttachRule().tagger, 1.01)  # é™„åŠ æ—¶é—´
        verb_duration = add_weight(VerbDurationRule().tagger, 1.02)  # åŠ¨è¯æŒç»­æ—¶é—´
        whitelist = add_weight(WhitelistRule().tagger, 1.03)  # ç™½åå•
        and_rule = add_weight(AndRule().tagger, 1.04)  # andè¿æ¥
        decimal = add_weight(DecimalRule().tagger, 1.3)  # å°æ•°

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ„å»ºskip_ruleä½¿ç”¨å›ºå®šçš„"none"è¾“å‡ºï¼Œé¿å…åŠ¨æ€æ·»åŠ ç¬¦å·
        sym = get_symbol_table()
        # æ„å»ºskip_ruleï¼šä½¿ç”¨è¯çº§tokenæ‹¼æ¥ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        from .word_level_pynini import pynutil as word_pynutil, accep

        # æ”¶é›†ç¬¦å·è¡¨ä¸­çš„æ‰€æœ‰tokenï¼ˆç§»é™¤é•¿åº¦é™åˆ¶ï¼Œæ”¯æŒå¤šå­—ç¬¦tokenï¼‰
        skip_arcs = []
        for idx in range(1, sym.num_symbols()):
            token = sym.find(idx)
            if (
                not token
                or token == "<eps>"
                or token.startswith("char { value:")
                # ç§»é™¤ len(token) > 1 é™åˆ¶ï¼Œæ”¯æŒæ‰€æœ‰é•¿åº¦çš„token
                # è¿™æ ·å¯ä»¥å…œåº•ç¬¦å·è¡¨ä¸­çš„å¤šå­—ç¬¦tokenï¼ˆå¦‚"to"ã€"tag"ç­‰ï¼‰ï¼Œé¿å…FSTå¤±è´¥
            ):
                continue

            # ä½¿ç”¨è¯çº§insertæ‹¼æ¥3ä¸ªtokenï¼š
            # 'char{value:"' + token + '"}'
            arc = word_pynutil.insert('char{value:"') + accep(token) + word_pynutil.insert('"}')
            skip_arcs.append(arc)

        # Unionæ‰€æœ‰tokençš„è§„åˆ™ï¼ˆä¸€æ¬¡æ€§unionï¼Œé¿å…O(nÂ²)å¤æ‚åº¦ï¼‰
        if skip_arcs:
            skip_rule = pynini.union(*skip_arcs).optimize()
        else:
            skip_rule = pynini.Fst()

        skip_rule = add_weight(skip_rule, 50)  # ä¸æ—§ç‰ˆæœ¬CharRuleå¯¹é½

        combined = (
            utctime
            | relative_day
            | period
            | delta
            | weekday
            | between
            | holiday
            | lunar
            | recurring
            | range_rule
            | unit
            | delta_time_attach
            | verb_duration
            | whitelist
            | and_rule
            | decimal
            | skip_rule
        ).optimize()

        # ä»ä»¥utcè§„åˆ™ä¸ºä¸»è¿›è¡ŒéªŒè¯ï¼Œåªå¯¹è¯¥ç»„åˆå–é—­åŒ…
        tagger = combined.star

        # ç®€åŒ–Îµå¹¶ä¿æŒFSTä¼˜åŒ–
        tagger = tagger.rmepsilon().optimize()

        # è¯çº§FSTï¼šé—­åŒ…åæ‰‹åŠ¨æ¢å¤ç¬¦å·è¡¨ï¼Œä¿æŒä¸è‹±æ–‡å®ç°ä¸€è‡´
        if self.use_word_level:
            sym = get_symbol_table()
            tagger.set_input_symbols(sym)
            tagger.set_output_symbols(sym)

        self.tagger = tagger
